#!/usr/bin/env python3
"""
LangGraph Agent for CrewCFO Planning & Spec Generation.
V2: Added save_to_file tool for direct file output.
"""
import os
import json
from pathlib import Path
from typing import Annotated, TypedDict, List, Optional
from datetime import datetime

import typer
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from dotenv import load_dotenv

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from retriever import get_or_create_vectorstore

load_dotenv()

console = Console()
app = typer.Typer()

# Output directory for generated files
GENERATED_DIR = Path("./docs/generated")


# =============================================================================
# STATE
# =============================================================================

class AgentState(TypedDict):
    messages: Annotated[List, add_messages]


# =============================================================================
# TOOLS
# =============================================================================

@tool
def search_knowledge_base(query: str) -> str:
    """Search the CrewCFO knowledge base for relevant documentation.
    
    Use this to find information about:
    - Valuation Intelligence Module (Features A-E)
    - Spec-OS modules (Torch, Takeoff, Ridgeline)
    - Books OS Constitution and automation tiers
    - Database schemas and data dependencies
    - Roofing industry metrics and benchmarks
    
    Args:
        query: Search query (e.g., "valuation driver scoring", "database schema")
    """
    vectorstore = get_or_create_vectorstore()
    results = vectorstore.similarity_search(query, k=5)
    
    if not results:
        return "No relevant documents found. Try a different query."
    
    formatted = []
    for i, doc in enumerate(results):
        source = doc.metadata.get("filename", "unknown")
        formatted.append(f"[Source: {source}]\n{doc.page_content}")
    
    return "\n\n---\n\n".join(formatted)


@tool
def save_to_file(filename: str, content: str, file_type: str = "md") -> str:
    """Save generated content to a file in docs/generated/.
    
    Use this after generating schemas, specs, epics, or any artifact the user wants saved.
    
    Args:
        filename: Name of the file (without extension), e.g., "01_database_schema"
        content: The content to save
        file_type: File extension - "sql", "yaml", "md", "json" (default: "md")
    
    Returns:
        Confirmation message with file path
    """
    GENERATED_DIR.mkdir(parents=True, exist_ok=True)
    
    # Clean filename
    safe_filename = "".join(c for c in filename if c.isalnum() or c in "._-")
    filepath = GENERATED_DIR / f"{safe_filename}.{file_type}"
    
    # Add header comment
    header = f"# Generated by CrewCFO Agent\n# Date: {datetime.now().isoformat()}\n# File: {filepath.name}\n\n"
    
    with open(filepath, "w") as f:
        if file_type in ["sql", "yaml", "py"]:
            comment_char = "--" if file_type == "sql" else "#"
            f.write(f"{comment_char} Generated by CrewCFO Agent\n")
            f.write(f"{comment_char} Date: {datetime.now().isoformat()}\n\n")
        elif file_type == "md":
            f.write(f"<!-- Generated by CrewCFO Agent - {datetime.now().isoformat()} -->\n\n")
        f.write(content)
    
    return f"✅ Saved to: {filepath.absolute()}"


@tool
def list_generated_files() -> str:
    """List all files in the docs/generated/ folder.
    
    Use this to show the user what has been generated.
    """
    if not GENERATED_DIR.exists():
        return "No generated files yet. docs/generated/ folder doesn't exist."
    
    files = list(GENERATED_DIR.glob("*"))
    if not files:
        return "No generated files yet."
    
    file_list = "\n".join([f"- {f.name} ({f.stat().st_size} bytes)" for f in sorted(files)])
    return f"Generated files in {GENERATED_DIR}:\n{file_list}"


@tool
def generate_database_schema(
    table_name: str,
    description: str,
    columns: List[str],
    indexes: Optional[List[str]] = None,
    foreign_keys: Optional[List[str]] = None
) -> str:
    """Generate a PostgreSQL schema definition for a CrewCFO table.
    
    Args:
        table_name: Name of the table (e.g., 'valuation_snapshots', 'driver_scores')
        description: What this table stores
        columns: List of column definitions (e.g., ['id UUID PRIMARY KEY', 'tenant_id UUID NOT NULL'])
        indexes: Optional list of index definitions
        foreign_keys: Optional list of foreign key constraints
    
    Returns:
        Complete SQL CREATE TABLE statement
    """
    columns_sql = ",\n  ".join(columns)
    
    schema = f"""-- {description}
-- Generated: {datetime.now().isoformat()}

CREATE TABLE {table_name} (
  {columns_sql}
);
"""
    
    if indexes:
        schema += "\n-- Indexes\n"
        for idx in indexes:
            schema += f"CREATE INDEX {idx};\n"
    else:
        schema += f"\n-- Default indexes\n"
        schema += f"CREATE INDEX idx_{table_name}_tenant ON {table_name}(tenant_id);\n"
        if "as_of_date" in columns_sql or "created_at" in columns_sql:
            date_col = "as_of_date" if "as_of_date" in columns_sql else "created_at"
            schema += f"CREATE INDEX idx_{table_name}_{date_col} ON {table_name}({date_col});\n"
    
    if foreign_keys:
        schema += "\n-- Foreign keys\n"
        for fk in foreign_keys:
            schema += f"ALTER TABLE {table_name} ADD CONSTRAINT {fk};\n"
    
    return schema


@tool
def generate_epic_breakdown(
    epic_name: str,
    description: str,
    user_stories: List[str],
    acceptance_criteria: List[str],
    estimated_hours: int,
    dependencies: Optional[List[str]] = None,
    phase: Optional[str] = None
) -> str:
    """Generate an Epic/Story breakdown in speckit.plan format.
    
    Args:
        epic_name: Name of the epic (e.g., 'Valuation Engine v1')
        description: What this epic delivers
        user_stories: List of user story descriptions
        acceptance_criteria: List of acceptance criteria
        estimated_hours: Total estimated hours
        dependencies: Optional list of dependent epics/features
        phase: Optional phase name (e.g., 'Phase 4: Valuation Intelligence')
    
    Returns:
        Markdown epic specification
    """
    phase_header = f"# {phase}\n\n" if phase else ""
    
    stories_md = "\n".join([f"  - [ ] {story}" for story in user_stories])
    criteria_md = "\n".join([f"  - [ ] {criterion}" for criterion in acceptance_criteria])
    deps_md = "\n".join([f"  - {dep}" for dep in (dependencies or ["None"])])
    
    return f"""{phase_header}## Epic: {epic_name}

**Description:** {description}

**Estimated Hours:** {estimated_hours}
**Status:** Not Started

### Dependencies
{deps_md}

### User Stories
{stories_md}

### Acceptance Criteria
{criteria_md}

### Technical Notes
- Follow Books OS Constitution automation tiers
- Human-in-loop for confidence < 80%
- Audit trail required for all valuation outputs
"""


@tool
def generate_feature_spec(
    feature_id: str,
    feature_name: str,
    purpose: str,
    inputs: List[str],
    outputs: List[str],
    automation_tier: str,
    data_sources: List[str],
    human_in_loop_rules: Optional[List[str]] = None
) -> str:
    """Generate a feature specification in YAML format for CrewCFO.
    
    Args:
        feature_id: Feature identifier (e.g., 'A', 'B', 'C')
        feature_name: Name of the feature (e.g., 'Real-time Valuation Engine')
        purpose: What this feature does
        inputs: List of required inputs/data
        outputs: List of outputs produced
        automation_tier: 'rules', 'ml', 'llm', or 'hybrid'
        data_sources: List of data sources (e.g., 'Books OS', 'QBO', 'Jobber')
        human_in_loop_rules: Optional list of human review rules
    
    Returns:
        YAML feature specification
    """
    inputs_yaml = "\n".join([f"    - {i}" for i in inputs])
    outputs_yaml = "\n".join([f"    - {o}" for o in outputs])
    sources_yaml = "\n".join([f"    - {s}" for s in data_sources])
    
    hitl_rules = human_in_loop_rules or [
        "Confidence < 80%: Flag for review",
        "Amount > $5,000: Require human approval",
        "Tier change: Require owner acknowledgment"
    ]
    hitl_yaml = "\n".join([f"    - {rule}" for rule in hitl_rules])
    
    return f"""# Feature {feature_id}: {feature_name}
# CrewCFO Valuation Intelligence Module

feature:
  id: "{feature_id}"
  name: "{feature_name}"
  purpose: "{purpose}"
  
  automation:
    tier: "{automation_tier}"
    confidence_threshold: 0.80
    
  data_sources:
{sources_yaml}
  
  inputs:
{inputs_yaml}
  
  outputs:
{outputs_yaml}
  
  human_in_loop:
{hitl_yaml}
  
  compliance:
    - "Books OS Constitution"
    - "Audit trail required"
    - "QBO as source of truth"
"""


@tool
def generate_implementation_roadmap(
    module_name: str,
    features: List[str],
    timeline_weeks: int,
    team_size: int
) -> str:
    """Generate an implementation roadmap for a CrewCFO module.
    
    Args:
        module_name: Name of the module (e.g., 'Valuation Intelligence')
        features: List of features to implement
        timeline_weeks: Total weeks for implementation
        team_size: Number of developers
    
    Returns:
        Markdown implementation roadmap
    """
    weeks_per_feature = max(1, timeline_weeks // len(features))
    
    phases = []
    current_week = 1
    
    for i, feature in enumerate(features):
        end_week = min(current_week + weeks_per_feature - 1, timeline_weeks)
        phases.append(f"""### Phase {i+1}: {feature}
**Timeline:** Weeks {current_week}-{end_week}

**Milestones:**
- [ ] Schema design and migrations
- [ ] Core logic implementation
- [ ] API endpoints
- [ ] UI components
- [ ] Integration tests
- [ ] Documentation
""")
        current_week = end_week + 1
    
    return f"""# {module_name} Implementation Roadmap

**Total Timeline:** {timeline_weeks} weeks
**Team Size:** {team_size} developers
**Features:** {len(features)}

---

{"".join(phases)}

---

## Success Criteria
- [ ] All features deployed to staging
- [ ] Performance benchmarks met
- [ ] Security audit passed
- [ ] User acceptance testing complete
- [ ] Documentation published

## Risk Mitigation
- Weekly progress reviews
- Feature flags for gradual rollout
- Rollback procedures documented
"""


@tool
def generate_api_design(
    resource_name: str,
    endpoints: List[str],
    base_path: str = "/api"
) -> str:
    """Generate API endpoint documentation.
    
    Args:
        resource_name: Name of the resource (e.g., 'valuation')
        endpoints: List of endpoint descriptions (e.g., ['GET /snapshot - Get latest snapshot'])
        base_path: API base path (default: /api)
    
    Returns:
        Markdown API documentation
    """
    endpoint_docs = []
    for ep in endpoints:
        parts = ep.split(" - ", 1)
        method_path = parts[0].strip()
        description = parts[1].strip() if len(parts) > 1 else ""
        
        method, path = method_path.split(" ", 1)
        full_path = f"{base_path}/{resource_name}{path}"
        
        endpoint_docs.append(f"""### {method} `{full_path}`

**Description:** {description}

**Request:**
```json
{{
  // Request body (if applicable)
}}
```

**Response:**
```json
{{
  "success": true,
  "data": {{}}
}}
```
""")
    
    return f"""# {resource_name.title()} API

Base URL: `{base_path}/{resource_name}`

---

{"".join(endpoint_docs)}

## Error Responses

All endpoints return errors in this format:
```json
{{
  "success": false,
  "error": {{
    "code": "ERROR_CODE",
    "message": "Human readable message"
  }}
}}
```
"""


@tool
def generate_data_model(
    entity_name: str,
    attributes: List[str],
    relationships: List[str]
) -> str:
    """Generate a data model diagram in Mermaid format.
    
    Args:
        entity_name: Main entity name (e.g., 'ValuationSnapshot')
        attributes: List of attributes (e.g., ['ttm_revenue: decimal', 'tier: string'])
        relationships: List of relationships (e.g., ['has_many DriverScores', 'belongs_to Tenant'])
    
    Returns:
        Mermaid ERD diagram
    """
    attrs = "\n    ".join([f"{attr.split(':')[0].strip()} {attr.split(':')[1].strip() if ':' in attr else 'string'}" for attr in attributes])
    
    rels = []
    for rel in relationships:
        if "has_many" in rel.lower():
            target = rel.replace("has_many", "").replace("has many", "").strip()
            rels.append(f"{entity_name} ||--o{{ {target} : has")
        elif "belongs_to" in rel.lower():
            target = rel.replace("belongs_to", "").replace("belongs to", "").strip()
            rels.append(f"{entity_name} }}o--|| {target} : belongs_to")
        elif "has_one" in rel.lower():
            target = rel.replace("has_one", "").replace("has one", "").strip()
            rels.append(f"{entity_name} ||--|| {target} : has")
    
    rel_lines = "\n  ".join(rels)
    
    return f"""```mermaid
erDiagram
  {entity_name} {{
    {attrs}
  }}
  
  {rel_lines}
```

## Entity: {entity_name}

### Attributes
{chr(10).join([f'- **{attr}**' for attr in attributes])}

### Relationships
{chr(10).join([f'- {rel}' for rel in relationships])}
"""


# =============================================================================
# AGENT
# =============================================================================

SYSTEM_PROMPT = """You are an expert technical architect and product planner for CrewCFO, 
an AI-powered bookkeeping and fractional CFO platform for roofing contractors.

You have access to these tools:

**Research:**
1. search_knowledge_base - Find information in project documentation

**Generation:**
2. generate_database_schema - Create PostgreSQL schema definitions
3. generate_epic_breakdown - Create Epic/Story breakdowns in speckit format
4. generate_feature_spec - Write YAML feature specifications
5. generate_implementation_roadmap - Create implementation timelines
6. generate_api_design - Create API endpoint documentation
7. generate_data_model - Create Mermaid ERD diagrams

**File Management:**
8. save_to_file - Save generated content to docs/generated/
9. list_generated_files - Show what files have been created

IMPORTANT WORKFLOW:
1. When asked to generate something, FIRST search the knowledge base for context
2. Generate the artifact using the appropriate tool
3. ALWAYS offer to save the output using save_to_file
4. Use descriptive filenames like "01_database_schema", "02_feature_a_spec", etc.

When helping with the Valuation Module, remember:
- 5 core features (A-E): Valuation Engine, Driver Scoring, Scenario Simulator, Exit Readiness, Roadmap Agent
- Matador tier multiples: Below Avg (~3×), Avg (~4.5-5×), Above Avg (~7×+)
- Automation tiers: Rules → ML → LLM (with confidence scoring)
- Human-in-loop required for: confidence < 80%, amounts > $5,000, tier changes
- QBO is source of truth (Books OS Constitution)

After generating content, ALWAYS ask: "Would you like me to save this to a file?"
"""


def create_agent():
    """Create the LangGraph agent with tools and memory."""
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not set")
    
    llm = ChatAnthropic(
        model=os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514"),
        api_key=api_key,
        max_tokens=8192,
        temperature=0.1
    )
    
    tools = [
        search_knowledge_base,
        save_to_file,
        list_generated_files,
        generate_database_schema,
        generate_epic_breakdown,
        generate_feature_spec,
        generate_implementation_roadmap,
        generate_api_design,
        generate_data_model,
    ]
    
    llm_with_tools = llm.bind_tools(tools)
    system_message = SystemMessage(content=SYSTEM_PROMPT)
    
    def call_model(state: AgentState):
        messages = [system_message] + state["messages"]
        response = llm_with_tools.invoke(messages)
        return {"messages": [response]}
    
    def should_continue(state: AgentState):
        last_message = state["messages"][-1]
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"
        return END
    
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", ToolNode(tools))
    
    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges("agent", should_continue, {"tools": "tools", END: END})
    workflow.add_edge("tools", "agent")
    
    memory = MemorySaver()
    
    return workflow.compile(checkpointer=memory)


# =============================================================================
# CLI
# =============================================================================

@app.command()
def chat(
    thread_id: str = typer.Option("crewcfo-planning", "--thread", "-t", help="Conversation thread ID"),
):
    """Interactive chat with the CrewCFO planning agent."""
    
    console.print(Panel.fit(
        "[bold blue]CrewCFO Planning Agent v2[/bold blue]\n"
        "I can generate specs AND save them to files.\n\n"
        "[dim]Commands: 'exit' to quit, 'files' to list generated files[/dim]",
        title="Welcome"
    ))
    
    try:
        agent = create_agent()
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)
    
    config = {"configurable": {"thread_id": thread_id}}
    
    while True:
        try:
            user_input = console.input("\n[bold cyan]You:[/bold cyan] ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ("exit", "quit", "q"):
                console.print("[yellow]Goodbye![/yellow]")
                break
            
            if user_input.lower() == "files":
                files = list(GENERATED_DIR.glob("*")) if GENERATED_DIR.exists() else []
                if files:
                    console.print(Panel("\n".join([f"• {f.name}" for f in sorted(files)]), title="Generated Files"))
                else:
                    console.print("[yellow]No files generated yet.[/yellow]")
                continue
            
            if user_input.lower() == "clear":
                config = {"configurable": {"thread_id": f"{thread_id}-{datetime.now().timestamp()}"}}
                console.print("[green]Conversation cleared.[/green]")
                continue
            
            with console.status("[bold green]Thinking..."):
                response = agent.invoke(
                    {"messages": [HumanMessage(content=user_input)]},
                    config=config
                )
            
            for msg in reversed(response["messages"]):
                if isinstance(msg, AIMessage) and msg.content:
                    console.print("\n[bold green]Agent:[/bold green]")
                    console.print(Markdown(msg.content))
                    break
                    
        except KeyboardInterrupt:
            console.print("\n[yellow]Interrupted. Type 'exit' to quit.[/yellow]")
        except Exception as e:
            console.print(f"[red]Error: {e}[/red]")


@app.command()
def task(
    query: str = typer.Argument(..., help="Task to perform"),
    save: bool = typer.Option(False, "--save", "-s", help="Auto-save outputs"),
):
    """Run a single task (non-interactive)."""
    
    try:
        agent = create_agent()
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)
    
    config = {"configurable": {"thread_id": f"task-{datetime.now().timestamp()}"}}
    
    # If save flag, append save instruction
    if save:
        query += "\n\nAfter generating, save the output to an appropriate file."
    
    with console.status("[bold green]Working..."):
        response = agent.invoke(
            {"messages": [HumanMessage(content=query)]},
            config=config
        )
    
    for msg in reversed(response["messages"]):
        if isinstance(msg, AIMessage) and msg.content:
            console.print(Markdown(msg.content))
            break


@app.command()
def generate_all():
    """Generate all specs for the Valuation Module and save to files."""
    
    console.print(Panel("[bold]Generating All Valuation Module Specs[/bold]", title="Batch Generation"))
    
    try:
        agent = create_agent()
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)
    
    tasks = [
        ("01_database_schema", "Generate complete PostgreSQL schemas for ALL valuation tables: valuation_snapshots, driver_scores, and roadmap_items. Include all columns, indexes, foreign keys, and constraints. Follow Books OS Constitution. Save to file 01_database_schema.sql"),
        ("02_feature_a_spec", "Generate a detailed feature specification for Feature A: Real-time Valuation Engine. Include inputs, outputs, data sources, automation tier, and human-in-loop rules. Save to file 02_feature_a_spec.yaml"),
        ("03_feature_b_spec", "Generate a detailed feature specification for Feature B: Value Driver Scoring Engine with the 6 Matador drivers. Save to file 03_feature_b_spec.yaml"),
        ("04_epic_breakdown", "Create a complete epic breakdown for Phase 4: Valuation Intelligence Module with all user stories for Features A-E. Save to file 04_epic_breakdown.md"),
        ("05_api_design", "Design all REST API endpoints for the Valuation Module including valuation snapshots, driver scores, scenario simulation, and roadmap items. Save to file 05_api_design.md"),
        ("06_implementation_roadmap", "Generate a 10-week implementation roadmap for the Valuation Intelligence Module with all 5 features. Save to file 06_implementation_roadmap.md"),
    ]
    
    for filename, task in tasks:
        console.print(f"\n[cyan]Generating: {filename}...[/cyan]")
        config = {"configurable": {"thread_id": f"batch-{filename}"}}
        
        with console.status("[bold green]Working..."):
            response = agent.invoke(
                {"messages": [HumanMessage(content=task)]},
                config=config
            )
        
        console.print(f"[green]✓ {filename} complete[/green]")
    
    console.print("\n" + "="*50)
    console.print("[bold green]All specs generated![/bold green]")
    console.print(f"Files saved to: {GENERATED_DIR.absolute()}")


if __name__ == "__main__":
    app()
